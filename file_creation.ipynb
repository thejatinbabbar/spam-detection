{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "file_creation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1lz7pu6SyatMEowXFjdONVz9Qf3xZnchY",
      "authorship_tag": "ABX9TyOVypmgpzblJsUED8FnHc09"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTJPz8dOIFHE",
        "outputId": "e9772a29-8d5c-46b3-b114-496f7b6ee85b"
      },
      "source": [
        "%%writefile /content/drive/MyDrive/data_mining/notebooks/feature_selection.py\n",
        "# importing modules\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "def import_files(data_path):\n",
        "    \"\"\"\n",
        "    Import the files and join the features with the labels to obtain a single dataframe\n",
        "    \"\"\"\n",
        "    users = os.path.join(data_path, 'users')\n",
        "    users_features = os.path.join(data_path, 'users_features')\n",
        "\n",
        "    coded_ids = pd.read_csv(os.path.join(users,'coded_ids.csv')).set_index('coded_id')\n",
        "    coded_ids_labels_train = pd.read_csv(os.path.join(users,'coded_ids_labels_train.csv')).set_index('coded_id')\n",
        "    coded_ids = coded_ids.join(coded_ids_labels_train)\n",
        "    coded_ids.reset_index(inplace=True)\n",
        "    coded_ids.set_index('user_id', inplace=True)\n",
        "\n",
        "    features = pd.read_csv(os.path.join(users_features, 'features.csv')).set_index('user_id')\n",
        "    # features_names = pd.read_csv(os.path.join(users_features, 'features_names.txt'), header=None)\n",
        "\n",
        "    data = features.join(coded_ids)\n",
        "    data.reset_index(inplace=True, drop=True)\n",
        "    data.set_index('coded_id', inplace=True)\n",
        "    data.sort_index(inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_clean_data(data_path):\n",
        "    \"\"\"\n",
        "    Clean the dataset by:\n",
        "\n",
        "    1. encode categorical variable\n",
        "    2. remove unnecessary features\n",
        "    3. fill null values\n",
        "    4. convert datetime to timestamp\n",
        "    \"\"\"\n",
        "    data = import_files(data_path)\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    data['lang'] = encoder.fit_transform(data['lang'])\n",
        "    data['time_zone'] = encoder.fit_transform(data['time_zone'].astype(str))\n",
        "    data['date_newest_tweet'] = pd.to_datetime(data['date_newest_tweet']).astype('int64') // 10 ** 9\n",
        "    data['date_oldest_tweet'] = pd.to_datetime(data['date_oldest_tweet']).astype('int64') // 10 ** 9\n",
        "    data['utc_offset'] = data['utc_offset'].fillna(0)\n",
        "\n",
        "    cols_to_remove = ['avg_intertweet_times',\n",
        "                      'max_intertweet_times',\n",
        "                      'min_intertweet_times',\n",
        "                      'std_intertweet_times',\n",
        "                      'followers_count_minus_2002',\n",
        "                      'friends_count_minus_2002',\n",
        "                      'spam_in_screen_name']\n",
        "    data.drop(cols_to_remove, axis=1, inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_feature_groups(data):\n",
        "    \"\"\"\n",
        "    Create different feature groups based on the correlation score with the target variable\n",
        "\n",
        "    1. all columns\n",
        "    2. columns with correlation score > 0.2\n",
        "    3. columns with correlation score > 0.3\n",
        "    4. top 30 columns with chi square score\n",
        "    5. top 50 columns with chi square score\n",
        "    6. top 80 columns with chi square score\n",
        "    \"\"\"\n",
        "    groups = []\n",
        "    groups.append(data.columns)\n",
        "    groups.append(data.corr()[data.corr().label.abs() > 0.2].label.index.values)\n",
        "    groups.append(data.corr()[data.corr().label.abs() > 0.3].label.index.values)\n",
        "    \n",
        "    scaler = MinMaxScaler()\n",
        "    data_new = scaler.fit_transform(data[data.label.notnull()].drop('label', axis=1))\n",
        "    selector = SelectKBest(chi2, k=30)\n",
        "    selector.fit(data_new, data[data.label.notnull()]['label'])\n",
        "    indices =[i for i, x in enumerate(selector.get_support()) if x]\n",
        "    group = [data.columns[i] for i in indices]\n",
        "    group.append('label')\n",
        "    groups.append(group)\n",
        "\n",
        "    selector = SelectKBest(chi2, k=50)\n",
        "    selector.fit(data_new, data[data.label.notnull()]['label'])\n",
        "    indices =[i for i, x in enumerate(selector.get_support()) if x]\n",
        "    group = [data.columns[i] for i in indices]\n",
        "    group.append('label')\n",
        "    groups.append(group)\n",
        "\n",
        "    selector = SelectKBest(chi2, k=80)\n",
        "    selector.fit(data_new, data[data.label.notnull()]['label'])\n",
        "    indices =[i for i, x in enumerate(selector.get_support()) if x]\n",
        "    group = [data.columns[i] for i in indices]\n",
        "    group.append('label')\n",
        "    groups.append(group)\n",
        "\n",
        "    return groups\n",
        "\n",
        "def get_train_and_val_set(data_path, return_full_set=False):\n",
        "    \"\"\"\n",
        "    Split the dataset into train set, validation set, and test set for each feature group\n",
        "    \"\"\"\n",
        "    data = get_clean_data(data_path)\n",
        "    groups = get_feature_groups(data)\n",
        "\n",
        "    train_set = []\n",
        "    train_val_set = []\n",
        "    for columns in groups:\n",
        "\n",
        "        train = data[columns][data.label.notnull()].copy()\n",
        "        train_set.append(train)\n",
        "        test = data[columns][data.label.isnull()].copy()\n",
        "        train, val = train_test_split(train, test_size=86, random_state=101)\n",
        "\n",
        "        X_train, y_train = train.drop('label', axis=1), train['label']\n",
        "        X_val, y_val = val.drop('label', axis=1), val['label']\n",
        "\n",
        "        train_val_set.append((X_train, y_train, X_val, y_val))\n",
        "\n",
        "    if return_full_set:\n",
        "        return train_val_set, train_set\n",
        "    else:\n",
        "        return train_val_set\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data_path = '/content/drive/MyDrive/data_mining/Social_spammers_dataset'\n",
        "    t_v_set = get_train_and_val_set(data_path)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/data_mining/notebooks/feature_selection.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EPH95GXVzgs"
      },
      "source": [
        "!python /content/drive/MyDrive/data_mining/notebooks/feature_selection.py"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kU1RDq8LEyq",
        "outputId": "ec04227b-71e5-4c03-a8a0-7bba9caeadaf"
      },
      "source": [
        "%%writefile /content/drive/MyDrive/data_mining/notebooks/models_with_crossval.py\n",
        "# importing modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import set_config\n",
        "from feature_selection import get_train_and_val_set\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
        "\n",
        "set_config(print_changed_only=True)\n",
        "\n",
        "def get_crossval_scores(data_path, to_print=False):\n",
        "    \"\"\"\n",
        "    Perform cross validation on the dataset and fit on different models\n",
        "    \"\"\"\n",
        "\n",
        "    models = [KNeighborsClassifier(n_neighbors=3),\n",
        "              DecisionTreeClassifier(),\n",
        "              RandomForestClassifier(n_estimators=10),\n",
        "              RandomForestClassifier(n_estimators=100),\n",
        "              GradientBoostingClassifier(n_estimators=10),\n",
        "              GradientBoostingClassifier(n_estimators=100)]\n",
        "    _, train_set = get_train_and_val_set(data_path, return_full_set=True)\n",
        "\n",
        "    scores = {}\n",
        "    for model in models:\n",
        "        scores[model] = {}\n",
        "\n",
        "    for model in models:\n",
        "        if to_print:\n",
        "            print(model)\n",
        "        for i, train in enumerate(train_set, 1):\n",
        "            X, y = train.drop('label', axis=1), train['label']\n",
        "            cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
        "            score = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "            scores[model][f'group {i}'] = score\n",
        "            if to_print:\n",
        "                print('---- feature group', i, ':', score, 'Mean:', np.mean(score))\n",
        "        print()\n",
        "\n",
        "    return scores\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data_path = '/content/drive/MyDrive/data_mining/Social_spammers_dataset'\n",
        "    scores = get_crossval_scores(data_path, to_print=True)\n",
        "    pd.DataFrame(scores).to_pickle('/content/drive/MyDrive/data_mining/notebooks/crossval_scores.pkl')"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/data_mining/notebooks/models_with_crossval.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEVB2RCTHYSD",
        "outputId": "ccbd07da-273f-4a73-98c2-fc6fca33eb23"
      },
      "source": [
        "!python /content/drive/MyDrive/data_mining/notebooks/models_with_crossval.py"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNeighborsClassifier(n_neighbors=3)\n",
            "---- feature group 1 : [0.9057971  0.86956522 0.88405797 0.87681159 0.87681159] Mean: 0.8826086956521738\n",
            "---- feature group 2 : [0.9057971  0.86956522 0.88405797 0.86231884 0.88405797] Mean: 0.881159420289855\n",
            "---- feature group 3 : [0.89130435 0.86231884 0.89855072 0.83333333 0.86956522] Mean: 0.8710144927536232\n",
            "---- feature group 4 : [0.84057971 0.84782609 0.82608696 0.85507246 0.82608696] Mean: 0.8391304347826087\n",
            "---- feature group 5 : [0.83333333 0.82608696 0.85507246 0.87681159 0.84057971] Mean: 0.846376811594203\n",
            "---- feature group 6 : [0.88405797 0.84057971 0.86231884 0.85507246 0.85507246] Mean: 0.8594202898550727\n",
            "\n",
            "DecisionTreeClassifier()\n",
            "---- feature group 1 : [0.97101449 0.92753623 0.94927536 0.97826087 0.94927536] Mean: 0.9550724637681158\n",
            "---- feature group 2 : [0.96376812 0.93478261 0.97101449 0.97101449 0.94202899] Mean: 0.9565217391304348\n",
            "---- feature group 3 : [0.97101449 0.96376812 0.97101449 0.96376812 0.95652174] Mean: 0.9652173913043478\n",
            "---- feature group 4 : [0.95652174 0.96376812 0.95652174 0.96376812 0.92753623] Mean: 0.9536231884057971\n",
            "---- feature group 5 : [0.95652174 0.97101449 0.97101449 0.97826087 0.94927536] Mean: 0.9652173913043478\n",
            "---- feature group 6 : [0.95652174 0.95652174 0.95652174 0.99275362 0.97101449] Mean: 0.9666666666666668\n",
            "\n",
            "RandomForestClassifier(n_estimators=10)\n",
            "---- feature group 1 : [0.94927536 0.94927536 0.98550725 0.97101449 0.97101449] Mean: 0.9652173913043478\n",
            "---- feature group 2 : [0.97826087 0.94202899 0.98550725 0.98550725 0.96376812] Mean: 0.9710144927536233\n",
            "---- feature group 3 : [0.97826087 0.94202899 0.97826087 0.98550725 0.94927536] Mean: 0.9666666666666666\n",
            "---- feature group 4 : [0.95652174 0.95652174 0.97826087 0.97826087 0.93478261] Mean: 0.9608695652173912\n",
            "---- feature group 5 : [0.97826087 0.94202899 0.97826087 0.98550725 0.95652174] Mean: 0.9681159420289855\n",
            "---- feature group 6 : [0.97826087 0.97826087 0.98550725 0.96376812 0.95652174] Mean: 0.972463768115942\n",
            "\n",
            "RandomForestClassifier()\n",
            "---- feature group 1 : [0.96376812 0.97101449 0.99275362 0.97101449 0.95652174] Mean: 0.9710144927536231\n",
            "---- feature group 2 : [0.97101449 0.95652174 0.97826087 0.99275362 0.95652174] Mean: 0.9710144927536233\n",
            "---- feature group 3 : [0.97826087 0.97101449 0.98550725 0.98550725 0.94927536] Mean: 0.9739130434782609\n",
            "---- feature group 4 : [0.97101449 0.96376812 0.99275362 1.         0.95652174] Mean: 0.9768115942028984\n",
            "---- feature group 5 : [0.97101449 0.96376812 0.97826087 0.99275362 0.94927536] Mean: 0.9710144927536233\n",
            "---- feature group 6 : [0.96376812 0.96376812 0.98550725 0.97826087 0.94927536] Mean: 0.9681159420289855\n",
            "\n",
            "GradientBoostingClassifier(n_estimators=10)\n",
            "---- feature group 1 : [0.95652174 0.94202899 0.96376812 0.97826087 0.94202899] Mean: 0.9565217391304348\n",
            "---- feature group 2 : [0.96376812 0.96376812 0.97101449 0.97826087 0.93478261] Mean: 0.9623188405797102\n",
            "---- feature group 3 : [0.95652174 0.94202899 0.97101449 0.98550725 0.92753623] Mean: 0.9565217391304348\n",
            "---- feature group 4 : [0.94927536 0.96376812 0.96376812 0.97826087 0.9057971 ] Mean: 0.9521739130434781\n",
            "---- feature group 5 : [0.94927536 0.94202899 0.94927536 0.98550725 0.91304348] Mean: 0.9478260869565217\n",
            "---- feature group 6 : [0.95652174 0.94927536 0.97101449 0.97826087 0.94927536] Mean: 0.9608695652173912\n",
            "\n",
            "GradientBoostingClassifier()\n",
            "---- feature group 1 : [0.97826087 0.97101449 0.97826087 1.         0.96376812] Mean: 0.9782608695652174\n",
            "---- feature group 2 : [0.97101449 0.97101449 0.97101449 0.99275362 0.94927536] Mean: 0.9710144927536233\n",
            "---- feature group 3 : [0.97826087 0.96376812 0.97826087 0.99275362 0.94927536] Mean: 0.972463768115942\n",
            "---- feature group 4 : [0.97826087 0.95652174 0.99275362 0.98550725 0.94202899] Mean: 0.9710144927536233\n",
            "---- feature group 5 : [0.97101449 0.96376812 0.97826087 1.         0.96376812] Mean: 0.9753623188405797\n",
            "---- feature group 6 : [0.97826087 0.97826087 0.99275362 0.98550725 0.94927536] Mean: 0.9768115942028986\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5xLFIqIIaN4",
        "outputId": "88f58c0d-fa15-4ca6-a159-83afdd2cba99"
      },
      "source": [
        "%%writefile /content/drive/MyDrive/data_mining/notebooks/ml_models.py\n",
        "# importing modules\n",
        "from feature_selection import get_train_and_val_set\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "def train(model, X_train, y_train):\n",
        "    \"\"\"\n",
        "    Fit the model and return the train score\n",
        "    \"\"\"\n",
        "    model.fit(X_train, y_train)\n",
        "    score = model.score(X_train, y_train)\n",
        "\n",
        "    return score\n",
        "\n",
        "def evaluate(model, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the validation set and return the val score\n",
        "    \"\"\"\n",
        "    score = model.score(X_val, y_val)\n",
        "\n",
        "    return score\n",
        "\n",
        "def predict(model, X_test):\n",
        "    \"\"\"\n",
        "    Predict the labels for the test set\n",
        "    \"\"\"\n",
        "    preds = model.predict(X_test)\n",
        "    \n",
        "    return preds\n",
        "\n",
        "data_path = '/content/drive/MyDrive/data_mining/Social_spammers_dataset'\n",
        "models = [DecisionTreeClassifier(),\n",
        "          RandomForestClassifier(),\n",
        "          GradientBoostingClassifier()]\n",
        "train_val_set = get_train_and_val_set(data_path)\n",
        "\n",
        "scores = {}\n",
        "for model in models:\n",
        "    scores[type(model).__name__] = {'train_scores': [], 'val_scores': []}\n",
        "\n",
        "for model in models:\n",
        "    print(type(model).__name__)\n",
        "    for i, (X_train, y_train, X_val, y_val) in enumerate(train_val_set, 1):\n",
        "        score = train(model, X_train, y_train)\n",
        "        scores[type(model).__name__]['train_scores'].append(score)\n",
        "        print('---- train score', i, ':', score)\n",
        "\n",
        "        score = evaluate(model, X_val, y_val)\n",
        "        scores[type(model).__name__]['val_scores'].append(score)\n",
        "        print('---- val score', i, ':', score)\n",
        "        print()\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/data_mining/notebooks/ml_models.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UE0nC7aMgBr",
        "outputId": "73822000-cf76-4a1a-df25-a4e37831ba6c"
      },
      "source": [
        "!python /content/drive/MyDrive/data_mining/notebooks/ml_models.py"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecisionTreeClassifier\n",
            "---- train score 1 : 1.0\n",
            "---- val score 1 : 0.9302325581395349\n",
            "\n",
            "---- train score 2 : 1.0\n",
            "---- val score 2 : 0.9534883720930233\n",
            "\n",
            "---- train score 3 : 1.0\n",
            "---- val score 3 : 0.9534883720930233\n",
            "\n",
            "---- train score 4 : 1.0\n",
            "---- val score 4 : 0.9651162790697675\n",
            "\n",
            "RandomForestClassifier\n",
            "---- train score 1 : 1.0\n",
            "---- val score 1 : 0.9767441860465116\n",
            "\n",
            "---- train score 2 : 1.0\n",
            "---- val score 2 : 0.9767441860465116\n",
            "\n",
            "---- train score 3 : 1.0\n",
            "---- val score 3 : 0.9767441860465116\n",
            "\n",
            "---- train score 4 : 1.0\n",
            "---- val score 4 : 0.9651162790697675\n",
            "\n",
            "GradientBoostingClassifier\n",
            "---- train score 1 : 1.0\n",
            "---- val score 1 : 0.9883720930232558\n",
            "\n",
            "---- train score 2 : 1.0\n",
            "---- val score 2 : 0.9767441860465116\n",
            "\n",
            "---- train score 3 : 1.0\n",
            "---- val score 3 : 0.9767441860465116\n",
            "\n",
            "---- train score 4 : 1.0\n",
            "---- val score 4 : 0.9651162790697675\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tHHRNBs1Vsj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}