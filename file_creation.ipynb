{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "file_creation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1lz7pu6SyatMEowXFjdONVz9Qf3xZnchY",
      "authorship_tag": "ABX9TyMyc0BPYgyCkxd4bPGMBKac"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTJPz8dOIFHE",
        "outputId": "e9772a29-8d5c-46b3-b114-496f7b6ee85b"
      },
      "source": [
        "%%writefile /content/drive/MyDrive/data_mining/notebooks/feature_selection.py\n",
        "# importing modules\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "def import_files(data_path):\n",
        "    \"\"\"\n",
        "    Import the files and join the features with the labels to obtain a single dataframe\n",
        "    \"\"\"\n",
        "    users = os.path.join(data_path, 'users')\n",
        "    users_features = os.path.join(data_path, 'users_features')\n",
        "\n",
        "    coded_ids = pd.read_csv(os.path.join(users,'coded_ids.csv')).set_index('coded_id')\n",
        "    coded_ids_labels_train = pd.read_csv(os.path.join(users,'coded_ids_labels_train.csv')).set_index('coded_id')\n",
        "    coded_ids = coded_ids.join(coded_ids_labels_train)\n",
        "    coded_ids.reset_index(inplace=True)\n",
        "    coded_ids.set_index('user_id', inplace=True)\n",
        "\n",
        "    features = pd.read_csv(os.path.join(users_features, 'features.csv')).set_index('user_id')\n",
        "    # features_names = pd.read_csv(os.path.join(users_features, 'features_names.txt'), header=None)\n",
        "\n",
        "    data = features.join(coded_ids)\n",
        "    data.reset_index(inplace=True, drop=True)\n",
        "    data.set_index('coded_id', inplace=True)\n",
        "    data.sort_index(inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_clean_data(data_path):\n",
        "    \"\"\"\n",
        "    Clean the dataset by:\n",
        "\n",
        "    1. encode categorical variable\n",
        "    2. remove unnecessary features\n",
        "    3. fill null values\n",
        "    4. convert datetime to timestamp\n",
        "    \"\"\"\n",
        "    data = import_files(data_path)\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    data['lang'] = encoder.fit_transform(data['lang'])\n",
        "    data['time_zone'] = encoder.fit_transform(data['time_zone'].astype(str))\n",
        "    data['date_newest_tweet'] = pd.to_datetime(data['date_newest_tweet']).astype('int64') // 10 ** 9\n",
        "    data['date_oldest_tweet'] = pd.to_datetime(data['date_oldest_tweet']).astype('int64') // 10 ** 9\n",
        "    data['utc_offset'] = data['utc_offset'].fillna(0)\n",
        "\n",
        "    cols_to_remove = ['avg_intertweet_times',\n",
        "                      'max_intertweet_times',\n",
        "                      'min_intertweet_times',\n",
        "                      'std_intertweet_times',\n",
        "                      'followers_count_minus_2002',\n",
        "                      'friends_count_minus_2002',\n",
        "                      'spam_in_screen_name']\n",
        "    data.drop(cols_to_remove, axis=1, inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_feature_groups(data):\n",
        "    \"\"\"\n",
        "    Create different feature groups based on the correlation score with the target variable\n",
        "\n",
        "    1. all columns\n",
        "    2. columns with correlation score > 0.2\n",
        "    3. columns with correlation score > 0.3\n",
        "    4. top 30 columns with chi square score\n",
        "    5. top 50 columns with chi square score\n",
        "    6. top 80 columns with chi square score\n",
        "    \"\"\"\n",
        "    groups = []\n",
        "    groups.append(data.columns)\n",
        "    groups.append(data.corr()[data.corr().label.abs() > 0.2].label.index.values)\n",
        "    groups.append(data.corr()[data.corr().label.abs() > 0.3].label.index.values)\n",
        "    \n",
        "    scaler = MinMaxScaler()\n",
        "    data_new = scaler.fit_transform(data[data.label.notnull()].drop('label', axis=1))\n",
        "    selector = SelectKBest(chi2, k=30)\n",
        "    selector.fit(data_new, data[data.label.notnull()]['label'])\n",
        "    indices =[i for i, x in enumerate(selector.get_support()) if x]\n",
        "    group = [data.columns[i] for i in indices]\n",
        "    group.append('label')\n",
        "    groups.append(group)\n",
        "\n",
        "    selector = SelectKBest(chi2, k=50)\n",
        "    selector.fit(data_new, data[data.label.notnull()]['label'])\n",
        "    indices =[i for i, x in enumerate(selector.get_support()) if x]\n",
        "    group = [data.columns[i] for i in indices]\n",
        "    group.append('label')\n",
        "    groups.append(group)\n",
        "\n",
        "    selector = SelectKBest(chi2, k=80)\n",
        "    selector.fit(data_new, data[data.label.notnull()]['label'])\n",
        "    indices =[i for i, x in enumerate(selector.get_support()) if x]\n",
        "    group = [data.columns[i] for i in indices]\n",
        "    group.append('label')\n",
        "    groups.append(group)\n",
        "\n",
        "    return groups\n",
        "\n",
        "def get_train_and_val_set(data_path, return_full_set=False):\n",
        "    \"\"\"\n",
        "    Split the dataset into train set, validation set, and test set for each feature group\n",
        "    \"\"\"\n",
        "    data = get_clean_data(data_path)\n",
        "    groups = get_feature_groups(data)\n",
        "\n",
        "    train_set = []\n",
        "    train_val_set = []\n",
        "    for columns in groups:\n",
        "\n",
        "        train = data[columns][data.label.notnull()].copy()\n",
        "        train_set.append(train)\n",
        "        test = data[columns][data.label.isnull()].copy()\n",
        "        train, val = train_test_split(train, test_size=86, random_state=101)\n",
        "\n",
        "        X_train, y_train = train.drop('label', axis=1), train['label']\n",
        "        X_val, y_val = val.drop('label', axis=1), val['label']\n",
        "\n",
        "        train_val_set.append((X_train, y_train, X_val, y_val))\n",
        "\n",
        "    if return_full_set:\n",
        "        return train_val_set, train_set\n",
        "    else:\n",
        "        return train_val_set\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data_path = '/content/drive/MyDrive/data_mining/Social_spammers_dataset'\n",
        "    t_v_set = get_train_and_val_set(data_path)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/data_mining/notebooks/feature_selection.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EPH95GXVzgs"
      },
      "source": [
        "!python /content/drive/MyDrive/data_mining/notebooks/feature_selection.py"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kU1RDq8LEyq",
        "outputId": "7bf01594-e385-410c-9b85-225c1dc8bc6d"
      },
      "source": [
        "%%writefile /content/drive/MyDrive/data_mining/notebooks/models_with_crossval.py\n",
        "# importing modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import set_config\n",
        "from feature_selection import get_train_and_val_set\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
        "\n",
        "set_config(print_changed_only=True)\n",
        "\n",
        "def get_crossval_scores(data_path, to_print=False):\n",
        "    \"\"\"\n",
        "    Perform cross validation on the dataset and fit on different models\n",
        "    \"\"\"\n",
        "\n",
        "    models = [KNeighborsClassifier(n_neighbors=5),\n",
        "              DecisionTreeClassifier(),\n",
        "              RandomForestClassifier(n_estimators=101),\n",
        "              RandomForestClassifier(n_estimators=200),\n",
        "              GradientBoostingClassifier(n_estimators=101),\n",
        "              GradientBoostingClassifier(n_estimators=200)]\n",
        "    _, train_set = get_train_and_val_set(data_path, return_full_set=True)\n",
        "\n",
        "    scores = {}\n",
        "    for model in models:\n",
        "        scores[model] = {}\n",
        "\n",
        "    for model in models:\n",
        "        if to_print:\n",
        "            print(model)\n",
        "        for i, train in enumerate(train_set, 1):\n",
        "            X, y = train.drop('label', axis=1), train['label']\n",
        "            cv = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n",
        "            score = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "            scores[model][f'group {i}'] = score\n",
        "            if to_print:\n",
        "                print('---- feature group', i, ':', score, 'Mean:', np.mean(score))\n",
        "        print()\n",
        "\n",
        "    return scores\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data_path = '/content/drive/MyDrive/data_mining/Social_spammers_dataset'\n",
        "    scores = get_crossval_scores(data_path, to_print=True)\n",
        "    pd.DataFrame(scores).to_pickle('/content/drive/MyDrive/data_mining/notebooks/crossval_scores.pkl')"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/data_mining/notebooks/models_with_crossval.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEVB2RCTHYSD",
        "outputId": "ee6fc4ae-b65f-4864-f589-7de89c4056d8"
      },
      "source": [
        "!python /content/drive/MyDrive/data_mining/notebooks/models_with_crossval.py"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNeighborsClassifier()\n",
            "---- feature group 1 : [0.91860465 0.86046512 0.90116279 0.86627907 0.85465116] Mean: 0.8802325581395349\n",
            "---- feature group 2 : [0.91860465 0.86046512 0.90116279 0.86627907 0.85465116] Mean: 0.8802325581395349\n",
            "---- feature group 3 : [0.87790698 0.84302326 0.90116279 0.83139535 0.84883721] Mean: 0.8604651162790697\n",
            "---- feature group 4 : [0.8372093  0.81395349 0.83139535 0.86627907 0.84883721] Mean: 0.8395348837209301\n",
            "---- feature group 5 : [0.8255814  0.80813953 0.84302326 0.86627907 0.86046512] Mean: 0.8406976744186047\n",
            "---- feature group 6 : [0.90116279 0.87790698 0.87790698 0.86627907 0.84883721] Mean: 0.8744186046511627\n",
            "\n",
            "DecisionTreeClassifier()\n",
            "---- feature group 1 : [0.95348837 0.93604651 0.94186047 0.97674419 0.95348837] Mean: 0.9523255813953488\n",
            "---- feature group 2 : [0.96511628 0.93023256 0.98255814 0.97093023 0.94767442] Mean: 0.9593023255813954\n",
            "---- feature group 3 : [0.97674419 0.90697674 0.96511628 0.94186047 0.93604651] Mean: 0.9453488372093023\n",
            "---- feature group 4 : [0.95930233 0.94186047 0.97093023 0.95348837 0.93604651] Mean: 0.9523255813953488\n",
            "---- feature group 5 : [0.96511628 0.93023256 0.97093023 0.97674419 0.93023256] Mean: 0.9546511627906977\n",
            "---- feature group 6 : [0.95348837 0.95348837 0.98255814 0.98255814 0.94767442] Mean: 0.963953488372093\n",
            "\n",
            "RandomForestClassifier(n_estimators=101)\n",
            "---- feature group 1 : [0.95348837 0.95348837 0.97674419 0.97093023 0.96511628] Mean: 0.963953488372093\n",
            "---- feature group 2 : [0.97093023 0.93604651 0.98255814 0.97093023 0.96511628] Mean: 0.9651162790697674\n",
            "---- feature group 3 : [0.97674419 0.9244186  0.98255814 0.98837209 0.97093023] Mean: 0.9686046511627907\n",
            "---- feature group 4 : [0.97093023 0.94186047 0.98255814 0.97674419 0.95930233] Mean: 0.9662790697674419\n",
            "---- feature group 5 : [0.97674419 0.94186047 0.98255814 0.98255814 0.96511628] Mean: 0.969767441860465\n",
            "---- feature group 6 : [0.95348837 0.95930233 0.98837209 0.96511628 0.96511628] Mean: 0.9662790697674419\n",
            "\n",
            "RandomForestClassifier(n_estimators=200)\n",
            "---- feature group 1 : [0.95348837 0.94186047 0.97674419 0.98255814 0.96511628] Mean: 0.963953488372093\n",
            "---- feature group 2 : [0.97093023 0.94186047 0.98255814 0.98837209 0.96511628] Mean: 0.969767441860465\n",
            "---- feature group 3 : [0.97674419 0.91860465 0.98837209 0.97674419 0.97093023] Mean: 0.9662790697674417\n",
            "---- feature group 4 : [0.97093023 0.94767442 0.97674419 0.98255814 0.95930233] Mean: 0.9674418604651163\n",
            "---- feature group 5 : [0.97093023 0.94767442 0.98837209 0.98255814 0.95930233] Mean: 0.969767441860465\n",
            "---- feature group 6 : [0.95348837 0.95348837 0.98837209 0.97674419 0.96511628] Mean: 0.9674418604651163\n",
            "\n",
            "GradientBoostingClassifier(n_estimators=101)\n",
            "---- feature group 1 : [0.97093023 0.96511628 0.98255814 0.98837209 0.95348837] Mean: 0.9720930232558139\n",
            "---- feature group 2 : [0.97093023 0.94767442 0.97093023 0.98837209 0.96511628] Mean: 0.9686046511627907\n",
            "---- feature group 3 : [0.97093023 0.93604651 0.97674419 0.97674419 0.96511628] Mean: 0.9651162790697674\n",
            "---- feature group 4 : [0.97674419 0.94767442 0.98255814 0.97674419 0.94767442] Mean: 0.9662790697674419\n",
            "---- feature group 5 : [0.96511628 0.94186047 0.97093023 0.98837209 0.97093023] Mean: 0.9674418604651163\n",
            "---- feature group 6 : [0.97093023 0.95930233 0.98255814 0.99418605 0.95930233] Mean: 0.9732558139534884\n",
            "\n",
            "GradientBoostingClassifier(n_estimators=200)\n",
            "---- feature group 1 : [0.97093023 0.95930233 0.97674419 0.98837209 0.95930233] Mean: 0.9709302325581396\n",
            "---- feature group 2 : [0.97093023 0.95930233 0.97093023 0.98837209 0.96511628] Mean: 0.9709302325581396\n",
            "---- feature group 3 : [0.97093023 0.93604651 0.98837209 0.97093023 0.96511628] Mean: 0.9662790697674419\n",
            "---- feature group 4 : [0.97674419 0.94186047 0.98255814 0.97674419 0.94767442] Mean: 0.9651162790697674\n",
            "---- feature group 5 : [0.96511628 0.94186047 0.97674419 0.98837209 0.97093023] Mean: 0.9686046511627907\n",
            "---- feature group 6 : [0.97093023 0.95930233 0.97674419 1.         0.96511628] Mean: 0.9744186046511627\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5xLFIqIIaN4",
        "outputId": "88f58c0d-fa15-4ca6-a159-83afdd2cba99"
      },
      "source": [
        "%%writefile /content/drive/MyDrive/data_mining/notebooks/ml_models.py\n",
        "# importing modules\n",
        "from feature_selection import get_train_and_val_set\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "def train(model, X_train, y_train):\n",
        "    \"\"\"\n",
        "    Fit the model and return the train score\n",
        "    \"\"\"\n",
        "    model.fit(X_train, y_train)\n",
        "    score = model.score(X_train, y_train)\n",
        "\n",
        "    return score\n",
        "\n",
        "def evaluate(model, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the validation set and return the val score\n",
        "    \"\"\"\n",
        "    score = model.score(X_val, y_val)\n",
        "\n",
        "    return score\n",
        "\n",
        "def predict(model, X_test):\n",
        "    \"\"\"\n",
        "    Predict the labels for the test set\n",
        "    \"\"\"\n",
        "    preds = model.predict(X_test)\n",
        "    \n",
        "    return preds\n",
        "\n",
        "data_path = '/content/drive/MyDrive/data_mining/Social_spammers_dataset'\n",
        "models = [DecisionTreeClassifier(),\n",
        "          RandomForestClassifier(),\n",
        "          GradientBoostingClassifier()]\n",
        "train_val_set = get_train_and_val_set(data_path)\n",
        "\n",
        "scores = {}\n",
        "for model in models:\n",
        "    scores[type(model).__name__] = {'train_scores': [], 'val_scores': []}\n",
        "\n",
        "for model in models:\n",
        "    print(type(model).__name__)\n",
        "    for i, (X_train, y_train, X_val, y_val) in enumerate(train_val_set, 1):\n",
        "        score = train(model, X_train, y_train)\n",
        "        scores[type(model).__name__]['train_scores'].append(score)\n",
        "        print('---- train score', i, ':', score)\n",
        "\n",
        "        score = evaluate(model, X_val, y_val)\n",
        "        scores[type(model).__name__]['val_scores'].append(score)\n",
        "        print('---- val score', i, ':', score)\n",
        "        print()\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/drive/MyDrive/data_mining/notebooks/ml_models.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UE0nC7aMgBr",
        "outputId": "73822000-cf76-4a1a-df25-a4e37831ba6c"
      },
      "source": [
        "!python /content/drive/MyDrive/data_mining/notebooks/ml_models.py"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecisionTreeClassifier\n",
            "---- train score 1 : 1.0\n",
            "---- val score 1 : 0.9302325581395349\n",
            "\n",
            "---- train score 2 : 1.0\n",
            "---- val score 2 : 0.9534883720930233\n",
            "\n",
            "---- train score 3 : 1.0\n",
            "---- val score 3 : 0.9534883720930233\n",
            "\n",
            "---- train score 4 : 1.0\n",
            "---- val score 4 : 0.9651162790697675\n",
            "\n",
            "RandomForestClassifier\n",
            "---- train score 1 : 1.0\n",
            "---- val score 1 : 0.9767441860465116\n",
            "\n",
            "---- train score 2 : 1.0\n",
            "---- val score 2 : 0.9767441860465116\n",
            "\n",
            "---- train score 3 : 1.0\n",
            "---- val score 3 : 0.9767441860465116\n",
            "\n",
            "---- train score 4 : 1.0\n",
            "---- val score 4 : 0.9651162790697675\n",
            "\n",
            "GradientBoostingClassifier\n",
            "---- train score 1 : 1.0\n",
            "---- val score 1 : 0.9883720930232558\n",
            "\n",
            "---- train score 2 : 1.0\n",
            "---- val score 2 : 0.9767441860465116\n",
            "\n",
            "---- train score 3 : 1.0\n",
            "---- val score 3 : 0.9767441860465116\n",
            "\n",
            "---- train score 4 : 1.0\n",
            "---- val score 4 : 0.9651162790697675\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "0tHHRNBs1Vsj",
        "outputId": "dd517a2e-15a1-4c02-b039-6c6aa5b5e755"
      },
      "source": [
        "from feature_selection import get_train_and_val_set"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-163-3551a4c1ca2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_train_and_val_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'feature_selection'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYEmxREDnt9H"
      },
      "source": [
        "!cd /content/drive/MyDrive/data_mining/notebooks/"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVvjLOQknwa3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}